
import torch

class MLP(torch.nn.Module):
    def __init__(self, in_feas, out_feas, device="cuda:0"):
        super(self.__class__, self).__init__()
        self.hid_num = 2048
        self.net = torch.nn.Sequential(
            torch.nn.Linear(in_features=in_feas, out_features=self.hid_num, bias=True),
            # torch.nn.ReLU(),
            torch.nn.Linear(in_features=self.hid_num,out_features=out_feas, bias=True),
        ).to(device)
    
    def forward(self, X): # X:[B,1,H,W]
        x = torch.nn.Flatten()(X) # [B,1,H,W] -> [B,d]
        return self.net(x)
    
    def fit(self, trainloader, lr=1e-3, weight_decay=2e-4, epoches=500, device="cuda:0"):
        print("...Training...")
        print("bs={:d}  lr={:.3f}  decay={:.4f}".format(trainloader.batch_size, lr, weight_decay))

        opt = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)
        for epoch in range(1,epoches+1):
            self.train()
            Tr_loss = 0
            for X, labels in trainloader:
                X,labels = X.to(device), labels.to(device) # X:[B,1,H,W]  labels:[B]
                y = self.forward(X) # pred_val:[B,cls]
                loss = torch.nn.CrossEntropyLoss()(y, labels)
                Tr_loss += loss.item()
                opt.zero_grad()
                loss.backward()
                opt.step()
            
            if epoch%10==0:
                torch.save(self.state_dict(), "./models/mlp/N"+str(trainloader.dataset.data.shape[0])\
                           +"_"+str(opt.__class__)[-6:-2]+"_bs"+str(trainloader.batch_size)+"_lr"+str(lr)\
                           +"_decay"+str(weight_decay)+"_eph"+str(epoch)+".pth")
                self.eval()
                with torch.no_grad():
                    total_correct_1, total_correct_5, total_num = 0.0, 0.0, 0
                    for X, labels in trainloader:
                        X, labels = X.to(device), labels.to(device)
                        total_num += X.size(dim=0)

                        pred_val = self.forward(X)
                        _, pred_cls = torch.topk(pred_val, k=5, dim=-1, largest=True)
                        top1_acc = torch.sum(( pred_cls[:,0] == labels ).float()).item() # item():tensor->float
                        top5_acc = torch.sum(( pred_cls[:,0:5] == labels.unsqueeze(-1).repeat(1,5) ).any(dim=-1)).item()
                        total_correct_1 += top1_acc
                        total_correct_5 += top5_acc
                    print("epoch{:3d}: InfoNCE Loss={:e} ".format(epoch, Tr_loss),
                          "Top1 acc={:.4f}%".format(total_correct_1/total_num*100),
                          "Top5 acc={:.4f}%".format(total_correct_5/total_num*100))


from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
train_dataset = MNIST(root="./dataset", train=True, transform=ToTensor(), download=True)
Ntr = 6000
torch.manual_seed(0)
chosen_tr = torch.randperm(train_dataset.data.size(dim=0))[:Ntr]

train_dataset.data = train_dataset.data[chosen_tr,:,:] # [Ntr, 28, 28] float32
train_dataset.targets = train_dataset.targets[chosen_tr]
train_dataset.targets.requires_grad = False
trainloader = torch.utils.data.DataLoader(train_dataset,batch_size=300, shuffle=True)

mlp = MLP(in_feas=784, out_feas=10)
mlp.fit(trainloader,lr=3e-4, weight_decay=2e-4, epoches=200, device="cuda:0")




bs=300  lr=0.000  decay=0.0002
epoch 10: InfoNCE Loss=5.330203e+00  Top1 acc=92.9833% Top5 acc=99.6833%
epoch 20: InfoNCE Loss=3.975683e+00  Top1 acc=94.9167% Top5 acc=99.7333%
epoch 30: InfoNCE Loss=3.235103e+00  Top1 acc=96.3167% Top5 acc=99.8500%
epoch 40: InfoNCE Loss=2.712839e+00  Top1 acc=97.1333% Top5 acc=99.8833%
epoch 50: InfoNCE Loss=2.293629e+00  Top1 acc=97.5333% Top5 acc=99.9500%
epoch 60: InfoNCE Loss=1.978522e+00  Top1 acc=98.0000% Top5 acc=99.9833%
epoch 70: InfoNCE Loss=1.707856e+00  Top1 acc=98.0333% Top5 acc=100.0000%
epoch 80: InfoNCE Loss=1.551446e+00  Top1 acc=98.8833% Top5 acc=100.0000%
epoch 90: InfoNCE Loss=1.300248e+00  Top1 acc=98.9333% Top5 acc=100.0000%
epoch100: InfoNCE Loss=1.149365e+00  Top1 acc=99.1500% Top5 acc=100.0000%
epoch110: InfoNCE Loss=1.069284e+00  Top1 acc=99.5167% Top5 acc=100.0000%
epoch120: InfoNCE Loss=9.502723e-01  Top1 acc=99.5333% Top5 acc=100.0000%
epoch130: InfoNCE Loss=8.431843e-01  Top1 acc=99.5833% Top5 acc=100.0000%
epoch140: InfoNCE Loss=7.379449e-01  Top1 acc=99.8500% Top5 acc=100.0000%
epoch150: InfoNCE Loss=6.817426e-01  Top1 acc=99.7833% Top5 acc=100.0000%
epoch160: InfoNCE Loss=6.634484e-01  Top1 acc=99.7167% Top5 acc=100.0000%
epoch170: InfoNCE Loss=6.836412e-01  Top1 acc=99.4833% Top5 acc=100.0000%
epoch180: InfoNCE Loss=5.601327e-01  Top1 acc=99.9000% Top5 acc=100.0000%
epoch190: InfoNCE Loss=5.007638e-01  Top1 acc=99.9500% Top5 acc=100.0000%
epoch200: InfoNCE Loss=5.825484e-01  Top1 acc=99.9833% Top5 acc=100.0000%
epoch210: InfoNCE Loss=5.012142e-01  Top1 acc=99.9500% Top5 acc=100.0000%
epoch220: InfoNCE Loss=5.217247e-01  Top1 acc=99.9333% Top5 acc=100.0000%
epoch230: InfoNCE Loss=5.372613e-01  Top1 acc=99.9167% Top5 acc=100.0000%
epoch240: InfoNCE Loss=4.974083e-01  Top1 acc=99.5833% Top5 acc=100.0000%